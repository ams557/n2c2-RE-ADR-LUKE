{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "# nltk.data.path.remove(\"/Users/ams557-macos/nltk_data\")\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.tokenize.punkt as pkt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, combinations\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os,sys, io\n",
    "from io import FileIO\n",
    "import fnmatch\n",
    "import re, string\n",
    "import csv\n",
    "from utils.helpers import *\n",
    "from tqdm import tqdm\n",
    "from transformers import LukeTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import LukeForEntityPairClassification, AdamW\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "wandb.login(key=\"3f4a097a574f34b0356bb664fb479ba2c4217659\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../DATA/'\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_DIR = DATA_DIR + 'train/training_20180910/'\n",
    "Path(TRAIN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR = DATA_DIR + 'test/test_data_Task2/'\n",
    "Path(TEST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvalidAnnotationError(ValueError):\n",
    "    pass\n",
    "\n",
    "def BRATtoDFconvert(path):\n",
    "    annotations = {\n",
    "        'entities' : pd.DataFrame(), \n",
    "        'relations' : pd.DataFrame()\n",
    "    }\n",
    "    files = [file for file in os.listdir(path) if file.endswith('.ann')]\n",
    "    files.sort(key=lambda f : os.path.splitext(f)[1])\n",
    "    for file in files:\n",
    "        annotation = read_file(path + '/' + file)\n",
    "        annotations['entities'] = pd.concat([annotations['entities'],process_annotation(path + file)['entities']],ignore_index=True) \n",
    "        annotations['relations'] = pd.concat([annotations['relations'],process_annotation(path + file)['relations']],ignore_index=True)\n",
    "    if not annotations['relations'].empty:\n",
    "        annotations['relations'].drop(columns=['tag'],inplace=True)\n",
    "        df = pd.merge(annotations['relations'],annotations['entities'][['file','tag','entity_span','entity']],left_on=['file','relation_start'],right_on=['file','tag'])\n",
    "        df.drop(columns=['tag','relation_start'],inplace=True)\n",
    "        df.rename(columns={'entity_span' : 'relation_start','entity' : 'start_entity', 'relation_name' : 'string_id'},inplace=True)\n",
    "        df = pd.merge(df,annotations['entities'][['file','tag','entity_span','entity']],left_on=['file','relation_end'],right_on=['file','tag'])\n",
    "        df.drop(columns=['tag','relation_end'],inplace=True)\n",
    "        df.rename(columns={'entity_span' : 'relation_end', 'entity' :'end_entity'},inplace=True)\n",
    "        df['entities'] = [[start, end] for start, end in zip(df['start_entity'], df['end_entity'])]\n",
    "        df.drop(columns=['start_entity','end_entity'],inplace=True)\n",
    "        df['original_article'] = [read_file(path + file + '.txt') for file in df['file']]\n",
    "        df.drop(columns='file')\n",
    "        df['start_idx'] = df.apply(lambda row : find_smallest_first_element(row, 'relation_start', 'relation_end'), axis=1)\n",
    "        df['end_idx'] = df.apply(lambda row : find_largest_last_element(row, 'relation_start', 'relation_end'), axis=1)\n",
    "        df['match'] = df.apply(lambda row : row['original_article'][row['start_idx']:row['end_idx']],axis=1)\n",
    "        df['sentences'] = df.apply(lambda row : find_sentences_around_match(text=row['original_article'],begin=row['start_idx'],end=row['end_idx']),axis=1)\n",
    "        df['BOS_idx'] = df.apply(lambda row : find_BOS_index(row['original_article'],row['start_idx']),axis=1)\n",
    "        df['entity_spans'] = df.apply(lambda row : np.array([norm_list(row['relation_start'],row['BOS_idx']),norm_list(row['relation_end'],row['BOS_idx'])],dtype=object),axis=1)\n",
    "        cols = ['end_idx', 'entities','entity_spans','match','original_article','sentences','start_idx','string_id']\n",
    "        df = df[cols]\n",
    "        return df.astype(object)\n",
    "    return annotations['entities']\n",
    "\n",
    "def grab_entity_info(line):\n",
    "    tags = line[1].split(\" \")\n",
    "    entity_name = str(tags[0])\n",
    "    entity_start = int(tags[1])\n",
    "    entity_end = int(tags[-1])\n",
    "    return pd.DataFrame({'tag' : line[0], 'entity_name' : entity_name, 'entity_span' : [np.array([entity_start, entity_end],dtype=object)], 'entity' : line[-1]},index=[0],dtype=object)\n",
    "\n",
    "def grab_relation_info(line):\n",
    "    tags = line[1].split(\" \")\n",
    "    assert len(tags) == 3, \"Incorrect relation format\"\n",
    "    relation_name = tags[0]\n",
    "    relation_start = tags[1].split(':')[1]\n",
    "    relation_end = tags[2].split(':')[1]\n",
    "    return pd.DataFrame({'tag' : line[0], 'relation_name' : relation_name, 'relation_start' : relation_start, 'relation_end' : relation_end},index=[0],dtype=object)\n",
    "\n",
    "def process_annotation(path):\n",
    "    annotations = {\n",
    "        'entities' : pd.DataFrame(), \n",
    "        'relations' : pd.DataFrame()\n",
    "    }\n",
    "    with open(path,'r') as file:\n",
    "        annotation = file.readlines()\n",
    "    for line in annotation:\n",
    "        line = line.strip()\n",
    "        annotations['entities']['file'] = os.path.split(path)[1].replace(\".ann\",\"\")\n",
    "        if line == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"\\t\" not in line:\n",
    "            InvalidAnnotationError(\"Line chunks in ANN files must be separated by tabs (See BRAT Guidelines).\")\n",
    "        line = line.split(\"\\t\")\n",
    "        if line[0][0] == 'T':\n",
    "            # print(f\"{os.path.split(path)[1].replace(\".ann\",\"\")}\")\n",
    "            annotations['entities'] = pd.concat([annotations['entities'],grab_entity_info(line)],ignore_index=True)\n",
    "        if line[0][0] == 'R':\n",
    "            # print(os.path.split(path)[1].replace(\".ann\",\"\"))\n",
    "            annotations['relations'] = pd.concat([annotations['relations'],grab_relation_info(line)],ignore_index=True)\n",
    "        annotations['relations']['file'] = os.path.split(path)[1].replace(\".ann\",\"\")\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = BRATtoDFconvert(TRAIN_DIR)\n",
    "train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_load.copy()\n",
    "train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sentences:', 'min =',str(train_df.sentences.str.len().min()) + ',','max =', str(train_df.sentences.str.len().max()))\n",
    "print('matches:','min =',str(train_df.match.str.len().min()) + ',','max =', str(train_df.match.str.len().max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = dict()\n",
    "for idx, label in enumerate(train_df.string_id.value_counts().index):\n",
    "  id2label[idx] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {v:k for k,v in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\", task=\"entity_pair_classification\")\n",
    "\n",
    "class RelationExtractionDataset(Dataset):\n",
    "    \"\"\"Relation extraction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data : Pandas dataframe.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "\n",
    "        sentences = item.sentences\n",
    "        entity_spans = [tuple(x) for x in item.entity_spans]\n",
    "\n",
    "        encoding = tokenizer(sentences, entity_spans=entity_spans, padding='max_length', truncation=True, return_tensors=\"pt\",max_length=257)\n",
    "\n",
    "        for k,v in encoding.items():\n",
    "          encoding[k] = encoding[k].squeeze()\n",
    "\n",
    "        encoding[\"label\"] = torch.tensor(label2id[item.string_id])\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df_load, test_size=0.2, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=RANDOM_STATE, shuffle=True)\n",
    "train_dataset = RelationExtractionDataset(data=train_df)\n",
    "valid_dataset = RelationExtractionDataset(data=val_df)\n",
    "# test_dataset = RelationExtractionDataset(data=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "tokenizer.decode(batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[batch[\"label\"][1].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LUKE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-base\", num_labels=len(label2id))\n",
    "\n",
    "    def forward(self, input_ids, entity_ids, entity_position_ids, attention_mask, entity_attention_mask):     \n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, entity_ids=entity_ids, \n",
    "                             entity_attention_mask=entity_attention_mask, entity_position_ids=entity_position_ids)\n",
    "        return outputs\n",
    "    \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        labels = batch['label']\n",
    "        del batch['label']\n",
    "        outputs = self(**batch)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss() # multi-class classification\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/batch['input_ids'].shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=5e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_dataloader))\n",
    "labels = batch[\"label\"]\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LUKE()\n",
    "del batch[\"label\"]\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "initial_loss = criterion(outputs.logits, labels)\n",
    "print(\"Initial loss:\", initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(name='luke-first-run-12000-articles-bis', project='LUKE')\n",
    "# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = Trainer(logger=wandb_logger, callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "predictions_total = []\n",
    "labels_total = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # get the inputs;\n",
    "    labels = batch[\"label\"]\n",
    "    del batch[\"label\"]\n",
    "\n",
    "    # move everything to the GPU\n",
    "    for k,v in batch.items():\n",
    "      batch[k] = batch[k].to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = loaded_model.model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1)\n",
    "    predictions_total.extend(predictions.tolist())\n",
    "    labels_total.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set:\", accuracy_score(labels_total, predictions_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LUKE.load_from_checkpoint(checkpoint_path=\"/content/drive/Shareddrives/Datascouts/epoch=3-step=7699.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "text = test_df.iloc[idx].sentence\n",
    "entity_spans = test_df.iloc[idx].entity_spans  # character-based entity spans\n",
    "entity_spans = [tuple(x) for x in entity_spans]\n",
    "\n",
    "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
    "\n",
    "outputs = loaded_model.model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Sentence:\", text)\n",
    "print(\"Ground truth label:\", test_df.iloc[idx].string_id)\n",
    "print(\"Predicted class idx:\", id2label[predicted_class_idx])\n",
    "print(\"Confidence:\", F.softmax(logits, -1).max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
